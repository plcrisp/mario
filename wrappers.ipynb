{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ.setdefault('PATH', '')\n",
    "import sys\n",
    "sys.path.append('D:/Anaconda3/envs/ambienteRL/Lib/site-packages') # Path to the gym library\n",
    "import gym\n",
    "import cv2\n",
    "cv2.ocl.setUseOpenCL(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to sample initial states by performing random no-op actions on reset.\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        \"\"\"Sample initial states by performing a random number of no-op actions on reset.\n",
    "        The no-op action is assumed to be action 0.\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Perform a no-op action for a random number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to handle environments that require an initial 'FIRE' action.\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Take action on reset for environments that require an initial 'FIRE' action.\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to make end-of-life equivalent to end-of-episode.\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life equivalent to end-of-episode, but only reset on true game over.\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipping\n",
    "\n",
    "Skipping refers to the process of reducing the frequency at which frames are processed by the agent. In environments like Atari games, consecutive frames often contain very little new information because the game state changes gradually over time. To address this redundancy, **frame skipping** is used.\n",
    "\n",
    "- **How it works**: The agent performs the same action for multiple consecutive frames (e.g., 4 frames) and only processes every `n`-th frame. Additionally, the maximum of the last two frames is often taken to capture any important changes that might have occurred during skipped frames.\n",
    "- **Benefits**:\n",
    "  - Reduces computational load by decreasing the number of frames the agent needs to process.\n",
    "  - Helps stabilize training by smoothing out small fluctuations in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to skip frames and take the maximum of the last two observations.\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame.\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clipping\n",
    "\n",
    "Clipping is a reward-shaping technique where the rewards returned by the environment are constrained to a fixed range, typically {-1, 0, +1}. This helps stabilize training in reinforcement learning algorithms.\n",
    "\n",
    "- **How it works**: Instead of using the raw rewards provided by the environment, the rewards are clipped based on their sign:\n",
    "  - Positive rewards are set to +1.\n",
    "  - Negative rewards are set to -1.\n",
    "  - Zero rewards remain unchanged.\n",
    "- **Why it's important**:\n",
    "  - Raw rewards in some environments can vary greatly in scale, which can destabilize training. For example, in a game where the score increases by large amounts, the agent might prioritize short-term gains over long-term strategies.\n",
    "  - Clipping ensures that all rewards are normalized, making it easier for the agent to learn stable policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to clip rewards to {-1, 0, +1}.\n",
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} based on its sign.\"\"\"\n",
    "        return np.sign(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to resize frames to 84x84 pixels and optionally convert to grayscale.\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\n",
    "        super().__init__(env)\n",
    "        self._width = width\n",
    "        self._height = height\n",
    "        self._grayscale = grayscale\n",
    "        self._key = dict_space_key\n",
    "        num_colors = 1 if self._grayscale else 3\n",
    "        new_space = gym.spaces.Box(low=0, high=255, shape=(self._height, self._width, num_colors), dtype=np.uint8)\n",
    "        if self._key is None:\n",
    "            self.observation_space = new_space\n",
    "        else:\n",
    "            self.observation_space.spaces[self._key] = new_space\n",
    "\n",
    "    def observation(self, obs):\n",
    "        if self._key is None:\n",
    "            frame = obs\n",
    "        else:\n",
    "            frame = obs[self._key]\n",
    "        if self._grayscale:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self._width, self._height), interpolation=cv2.INTER_AREA)\n",
    "        if self._grayscale:\n",
    "            frame = np.expand_dims(frame, -1)\n",
    "        return frame if self._key is None else obs.copy().update({self._key: frame})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "\n",
    "Stacking involves combining multiple consecutive frames into a single observation. This technique provides the agent with temporal context, allowing it to infer motion or changes in the environment over time.\n",
    "\n",
    "- **How it works**: Instead of using a single frame as input, the agent receives a stack of `k` consecutive frames (e.g., 4 frames). These frames are concatenated along a specific axis to form a multi-channel observation.\n",
    "- **Why it's important**:\n",
    "  - Single frames lack information about velocity, direction, or other temporal dynamics. For example, in a game like Pong, knowing the ball's current position isn't enough; the agent also needs to know its direction of movement.\n",
    "  - By stacking frames, the agent can \"see\" how objects move over time, enabling it to make more informed decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to stack the last `k` frames together.\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)), dtype=env.observation_space.dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        return np.concatenate(list(self.frames), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to scale pixel values to [0, 1].\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility class to optimize memory usage by storing shared frames once.\n",
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        self._frames = frames\n",
    "        self._out = None\n",
    "\n",
    "    def _force(self):\n",
    "        if self._out is None:\n",
    "            self._out = np.concatenate(self._frames, axis=-1)\n",
    "            self._frames = None\n",
    "        return self._out\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        out = self._force()\n",
    "        return out.astype(dtype) if dtype is not None else out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._force())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._force()[i]\n",
    "\n",
    "    def count(self):\n",
    "        frames = self._force()\n",
    "        return frames.shape[frames.ndim - 1]\n",
    "\n",
    "    def frame(self, i):\n",
    "        return self._force()[..., i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply a series of wrappers for preprocessing the environment.\n",
    "def custom_wrapper(env):\n",
    "    \"\"\"\n",
    "    Apply a series of wrappers to preprocess the environment for reinforcement learning tasks.\n",
    "    This function applies the following transformations:\n",
    "    \n",
    "    1. MaxAndSkipEnv: Skips every 4 frames and takes the maximum of the last two frames.\n",
    "    2. WarpFrame: Resizes the observation frames to 84x84 pixels and converts them to grayscale.\n",
    "    3. FrameStack: Stacks the last 4 frames together to provide temporal context to the agent.\n",
    "    4. ClipRewardEnv: Clips the rewards to {-1, 0, +1} to reduce the scale of rewards and stabilize training.\n",
    "    \n",
    "    Args:\n",
    "        env (gym.Env): The original Gym environment to wrap.\n",
    "    \n",
    "    Returns:\n",
    "        gym.Env: A wrapped environment with the specified preprocessing applied.\n",
    "    \"\"\"\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    env = WarpFrame(env)\n",
    "    env = FrameStack(env, 4)\n",
    "    env = ClipRewardEnv(env)\n",
    "    return env"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambienteRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
